# Consistent Hashing
As mentioned earlier that Bulkhead pattern exhibits a principle of damage containment by partitioning the overall system. With sharded/partitioned services, each replica, or shard, is only capable of serving a subset of all requests. A load-balancing node, or root, is responsible for examining each request and distributing each request to the appropriate shard or shards for processing.

Setting up the initial shards for a new partitioned service is relatively straightforward: you set up the appropriate shards and the roots to perform the sharding. However, what happens when you need to change the number of shards in your sharded service (re-balancing)? Such “re-sharding” is often a complicated process.

Let’s assume there are four servers in the network: Server-1, Server-2, Server-3 and Server-4. All four servers are identical, but have no knowledge of each other. To keep things simple in this example the keys are incrementing integers. Typically, you run the key against a checksum hash function to return a number. Once you have that number, you can take the modulo of that number against the number of nodes. This works surprisingly when the network is stable, i.e. no nodes are leaving or joining the network.

Now imagine, if a server, i.e. server-2 goes down. Then we’ve a big problem. Using the same hash function, we get the same result, but apply modulo operation we get different results than before, since the number of nodes is reduced by one. However, it is noteworthy here that nearly all the keys from all nodes need to be remapped as well. This make no sense, why should the keys that in servers that are functioning properly have to be remapped.

In a sharded cache system, for example, this is going to dramatically increase your miss rate until the cache is repopulated with responses for the new requests that have been mapped to that cache shard by the new sharding function. In the worst case, rolling out a new sharding function for your sharded cache will be equivalent to a complete cache failure.

This is where consistent hashing scheme comes into play. To resolve these kinds of problems, many sharding functions use consistent hashing functions. Consistent hashing is a special kind of hashing such that when a hash table is resized and consistent hashing is used, only K/n keys need to be remapped on average, where K is the number of keys, and n is the number of slots. If we had used consistent hashing above, then only the keys from server-2 need to be moved around.

Consistent hashing functions are essentially special hash functions that are guaranteed to only remap # keys / # shards, when being resized to # shards. Similarly, while adding a new server in a sharded/partitioned system, if we use a consistent hashing function for our sharded service, moving from 10 to 11 shards will only result in remapping < 10% (K / 11) keys. This is dramatically better than losing the entire sharded service.